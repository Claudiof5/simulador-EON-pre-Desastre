{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validação do Sistema de Pesos\n",
    "\n",
    "Este notebook valida o sistema de pesos proposto para roteamento:\n",
    "\n",
    "1. **ISP Usage Weight** (1.0-1.2): Balanceamento interno\n",
    "2. **Migration Weight** (0.0-0.2): Evitar conflito com migrações\n",
    "3. **Link Criticality Weight** (0.0-0.4): Proteger recursos críticos\n",
    "\n",
    "**Range Total:** 1.0 a 1.8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Carregar Cenário\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load scenario\n",
    "from simulador.core.topology import Topology\n",
    "from simulador.entities.isp import ISP\n",
    "\n",
    "\n",
    "scenario_path = Path(\"output/cenario1.pkl\")\n",
    "\n",
    "with open(scenario_path, 'rb') as f:\n",
    "    cenario = pickle.load(f)\n",
    "\n",
    "topology: Topology = cenario.topology\n",
    "lista_de_isps: list[ISP] = topology.lista_de_isps\n",
    "disaster_node = 9\n",
    "\n",
    "alfa = 0.2\n",
    "beta = 0.2\n",
    "gamma = 0.4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Calcular ISP Usage Weights\n",
    "\n",
    "Para cada ISP, calcula a frequência com que cada link aparece em shortest paths internos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simulador.entities.isp import ISP\n",
    "\n",
    "\n",
    "def calculate_isp_usage_weights(isp_list:list[ISP], alfa:float = 0.2):\n",
    "    \"\"\"Calculate ISP usage weights based on shortest path frequency.\"\"\"\n",
    "    \n",
    "    link_ocurrances_in_all_isps = defaultdict(int)\n",
    "    for isp in isp_list:\n",
    "        \n",
    "        for edge in isp.edges:\n",
    "            link_ocurrances_in_all_isps[edge] += 1\n",
    "    \n",
    "    weights_per_isp = { isp.isp_id: {} for isp in isp_list }\n",
    "    for isp in isp_list:\n",
    "        for edge in isp.edges:\n",
    "            normalized = alfa*(link_ocurrances_in_all_isps[edge] -1)/ (len(isp_list) -1)\n",
    "            weights_per_isp[isp.isp_id][edge] = normalized\n",
    "            reverse_edge = (edge[1], edge[0])\n",
    "            weights_per_isp[isp.isp_id][reverse_edge] = normalized\n",
    "    \n",
    "    return weights_per_isp\n",
    "\n",
    "isp_usage_weights = calculate_isp_usage_weights(lista_de_isps, alfa)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Calcular Migration Weights\n",
    "\n",
    "Baseado em quais links são usados pelos paths de migração de datacenters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_migration_weights(lista_de_isps: list[ISP], beta: float = 0.2):\n",
    "    \"\"\"Calculate migration weights based on datacenter migration paths.\"\"\"\n",
    "    \n",
    "    link_count = defaultdict(int)\n",
    "    total_migration_paths = 0\n",
    "    \n",
    "    for isp in lista_de_isps:\n",
    "        if not hasattr(isp, 'datacenter') or isp.datacenter is None:\n",
    "            continue\n",
    "        \n",
    "        datacenter = isp.datacenter\n",
    "        src = datacenter.source\n",
    "        dst = datacenter.destination\n",
    "        \n",
    "        # Get paths from ISP's internal paths\n",
    "        if hasattr(isp, 'caminhos_internos_isp') and isp.caminhos_internos_isp:\n",
    "            if src in isp.caminhos_internos_isp and dst in isp.caminhos_internos_isp[src]:\n",
    "                paths = isp.caminhos_internos_isp[src][dst]\n",
    "                \n",
    "                for path_info in paths:\n",
    "                    caminho = path_info[\"caminho\"]\n",
    "                    total_migration_paths += 1\n",
    "                    \n",
    "                    for i in range(len(caminho) - 1):\n",
    "                        link = (caminho[i], caminho[i + 1])\n",
    "                        link_count[link] += 1\n",
    "                        \n",
    "                        reverse = (caminho[i + 1], caminho[i])\n",
    "                        link_count[reverse] += 1\n",
    "    \n",
    "    if not link_count:\n",
    "        return {}\n",
    "    \n",
    "    max_count = max(link_count.values())\n",
    "    migration_weights = {}\n",
    "    \n",
    "    for link, count in link_count.items():\n",
    "        normalized = count / max_count if max_count > 0 else 0\n",
    "        weight = normalized * beta\n",
    "        migration_weights[link] = weight\n",
    "    \n",
    "    return migration_weights\n",
    "\n",
    "\n",
    "migration_weights = calculate_migration_weights(lista_de_isps, beta)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Calcular Link Criticality Weights\n",
    "\n",
    "Baseado em quantas ISPs dependem de cada link (bridges).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_link_criticality(topology, disaster_node, gamma: float = 0.4):\n",
    "    \"\"\"Calculate link criticality based on bridges across all ISPs.\"\"\"\n",
    "    \n",
    "    link_criticality = {}\n",
    "    \n",
    "    for isp in topology.lista_de_isps:\n",
    "        # Create ISP subgraph\n",
    "        isp_graph = topology.topology.subgraph(isp.nodes).copy()\n",
    "        for edge in isp.edges:\n",
    "            if (edge[0] in isp.nodes and edge[1] in isp.nodes \n",
    "                and not isp_graph.has_edge(edge[0], edge[1])\n",
    "                and topology.topology.has_edge(edge[0], edge[1])):\n",
    "                edge_data = topology.topology[edge[0]][edge[1]].copy()\n",
    "                isp_graph.add_edge(edge[0], edge[1], **edge_data)\n",
    "        \n",
    "        # Remove disaster node\n",
    "        if disaster_node in isp_graph.nodes():\n",
    "            isp_graph.remove_node(disaster_node)\n",
    "        \n",
    "        # Find bridges\n",
    "        bridges = list(nx.bridges(isp_graph))\n",
    "        \n",
    "        for link in bridges:\n",
    "            # Forward direction\n",
    "            if link not in link_criticality:\n",
    "                link_criticality[link] = {\n",
    "                    'bridge_count': 0,\n",
    "                    'isp_list': [],\n",
    "                    'weight_penalty': 0.0,\n",
    "                }\n",
    "            link_criticality[link]['bridge_count'] += 1\n",
    "            link_criticality[link]['isp_list'].append(isp.isp_id)\n",
    "            \n",
    "            # Reverse direction\n",
    "            reverse = (link[1], link[0])\n",
    "            if reverse not in link_criticality:\n",
    "                link_criticality[reverse] = {\n",
    "                    'bridge_count': 0,\n",
    "                    'isp_list': [],\n",
    "                    'weight_penalty': 0.0,\n",
    "                }\n",
    "            link_criticality[reverse]['bridge_count'] += 1\n",
    "            link_criticality[reverse]['isp_list'].append(isp.isp_id)\n",
    "    \n",
    "    \n",
    "    link_criticality_return = {}\n",
    "    for link, data in link_criticality.items():\n",
    "        normalized = (data['bridge_count'] )\n",
    "        link_criticality_return[link] = normalized/len(lista_de_isps)*gamma\n",
    "    \n",
    "    return link_criticality_return\n",
    "\n",
    "\n",
    "link_criticality_weights = calculate_link_criticality(topology, disaster_node, gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Exemplo: Calcular Peso Total de um Link\n",
    "\n",
    "Vamos calcular o peso total para um link específico em diferentes ISPs para validar a lógica condicional.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_weights_by_isps(\n",
    "    lista_de_isps: list[ISP],\n",
    "    isp_usage_weights: dict,\n",
    "    migration_weights: dict,\n",
    "    link_criticality_weights: dict\n",
    "    ) -> dict[int, dict[tuple[int, int], dict[str, float]]]:\n",
    "    link_weights_by_isp: dict[int, dict[tuple[int, int], dict[str, float]]] = {}\n",
    "    for isp in lista_de_isps:\n",
    "        isp_id = isp.isp_id\n",
    "        link_weights_by_isp[isp.isp_id] = {}\n",
    "        for link in isp.edges:\n",
    "            reverse_link = (link[1], link[0])\n",
    "            isp_weight = isp_usage_weights.get(isp_id, {}).get(link, 0.0)\n",
    "            if isp_weight == 1.0:  # Try reverse\n",
    "                isp_weight = isp_usage_weights.get(isp_id, {}).get(reverse_link, 0.0)\n",
    "            \n",
    "            migration_weight = migration_weights.get(link, 0.0)\n",
    "            if migration_weight == 0.0:  # Try reverse\n",
    "                migration_weight = migration_weights.get(reverse_link, 0.0)\n",
    "            \n",
    "            criticality_weight = link_criticality_weights.get(link, 0.0)\n",
    "            if criticality_weight == 0.0:  # Try reverse\n",
    "                criticality_weight = link_criticality_weights.get(reverse_link, 0.0)\n",
    "            \n",
    "            link_components = {\n",
    "                    'isp_usage': isp_weight,\n",
    "                    'migration': migration_weight,\n",
    "                    'criticality': criticality_weight,\n",
    "                    'total': 1 + isp_weight + migration_weight + criticality_weight\n",
    "                }\n",
    "            total_multiplier = 1 + isp_weight + migration_weight + criticality_weight\n",
    "            link_weights_by_isp[isp.isp_id][link] = link_components\n",
    "            link_weights_by_isp[isp.isp_id][reverse_link] = link_components\n",
    "\n",
    "    return link_weights_by_isp\n",
    "\n",
    "weights_by_link_by_isp = calculate_weights_by_isps(lista_de_isps, isp_usage_weights, migration_weights, link_criticality_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "\n",
    "def create_weighted_graph(\n",
    "    graph: nx.Graph,\n",
    "    isp_id: int,\n",
    "    weights_by_link_by_isp: dict[int, dict[tuple[int, int], dict[str, float]]]\n",
    ") -> nx.Graph:\n",
    "    \"\"\"Create a copy of the graph with modified edge weights.\n",
    "    \n",
    "    Each edge weight is multiplied by the total weight from 3 components:\n",
    "    - ISP Usage Weight (0.0-0.alfa)\n",
    "    - Migration Weight (0.0-0.beta)  \n",
    "    - Link Criticality Weight (0.0-0.gamma, conditional on ISP)\n",
    "    \n",
    "    Args:\n",
    "        graph: Original NetworkX graph\n",
    "        isp_id: ID of the ISP requesting paths\n",
    "        isp_usage_weights: dict[isp_id]['weights'][link] = weight\n",
    "        migration_weights: dict[link] = weight\n",
    "        link_criticality_weights: dict[link]['penalty_per_isp'][isp_id] = weight\n",
    "        \n",
    "    Returns:\n",
    "        Graph with modified edge weights\n",
    "    \"\"\"\n",
    "    weighted_graph = graph.copy()\n",
    "    \n",
    "    for u, v, data in weighted_graph.edges(data=True):\n",
    "        link = (u, v)\n",
    "        \n",
    "        total_multiplier = weights_by_link_by_isp.get(isp_id, {}).get(link, {}).get('total', 1.0)\n",
    "        # Apply to edge weight (distance)\n",
    "        original_weight = data.get('weight', 1.0)\n",
    "        data['weight'] = original_weight * total_multiplier\n",
    "    \n",
    "    return weighted_graph\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_isp_topology_with_weights(\n",
    "    isp, \n",
    "    topology_graph, \n",
    "    disaster_node,\n",
    "    isp_id,\n",
    "    weights_by_link_by_isp,\n",
    "    figsize=(18, 10)\n",
    "):\n",
    "    \"\"\"\n",
    "    Visualize FULL topology with ISP nodes/links highlighted and colored by weight.\n",
    "    \n",
    "    Args:\n",
    "        isp: ISP object\n",
    "        topology_graph: Main topology graph (full network)\n",
    "        disaster_node: Node to remove (disaster)\n",
    "        isp_id: ID of the ISP\n",
    "        isp_weights: ISP usage weights dict\n",
    "        migration_weights: Migration weights dict\n",
    "        link_criticality: Link criticality dict\n",
    "        figsize: Figure size tuple\n",
    "    \"\"\"\n",
    "    # Get ISP nodes and edges\n",
    "    isp_nodes_set = set(isp.nodes)\n",
    "    isp_edges_set = set()\n",
    "    \n",
    "    # Build ISP edges set\n",
    "    for edge in isp.edges:\n",
    "        if edge[0] in isp_nodes_set and edge[1] in isp_nodes_set:\n",
    "            isp_edges_set.add(edge)\n",
    "            isp_edges_set.add((edge[1], edge[0]))  # Add reverse\n",
    "    \n",
    "    # Remove disaster node from ISP nodes if present\n",
    "    isp_nodes_active = isp_nodes_set - {disaster_node}\n",
    "    \n",
    "    # Check if ISP is connected after disaster\n",
    "    isp_graph = topology_graph.subgraph(isp_nodes_active).copy()\n",
    "    for edge in isp.edges:\n",
    "        if (edge[0] in isp_nodes_active and edge[1] in isp_nodes_active \n",
    "            and not isp_graph.has_edge(edge[0], edge[1])\n",
    "            and topology_graph.has_edge(edge[0], edge[1])):\n",
    "            edge_data = topology_graph[edge[0]][edge[1]].copy()\n",
    "            isp_graph.add_edge(edge[0], edge[1], **edge_data)\n",
    "    \n",
    "    is_connected = nx.is_connected(isp_graph)\n",
    "    if not is_connected:\n",
    "        print(f\"⚠️  ISP {isp_id} está particionada após o desastre!\")\n",
    "        components = list(nx.connected_components(isp_graph))\n",
    "        print(f\"   Componentes: {len(components)}\")\n",
    "        for i, comp in enumerate(components, 1):\n",
    "            print(f\"   - Componente {i}: {len(comp)} nós\")\n",
    "    \n",
    "    # Calculate link frequency in shortest paths (for thickness)\n",
    "    # Calculate within each connected component\n",
    "    link_frequency = defaultdict(int)\n",
    "    if is_connected:\n",
    "        # Single component - calculate for all nodes\n",
    "        nodes = sorted(isp_graph.nodes())\n",
    "        for src in nodes:\n",
    "            for dst in nodes:\n",
    "                if src >= dst:\n",
    "                    continue\n",
    "                try:\n",
    "                    path = nx.shortest_path(isp_graph, src, dst, weight='weight')\n",
    "                    for i in range(len(path) - 1):\n",
    "                        link = tuple(sorted([path[i], path[i + 1]]))\n",
    "                        link_frequency[link] += 1\n",
    "                except nx.NetworkXNoPath:\n",
    "                    continue\n",
    "    else:\n",
    "        # Multiple components - calculate within each component\n",
    "        for component in nx.connected_components(isp_graph):\n",
    "            comp_nodes = sorted(component)\n",
    "            for src in comp_nodes:\n",
    "                for dst in comp_nodes:\n",
    "                    if src >= dst:\n",
    "                        continue\n",
    "                    try:\n",
    "                        path = nx.shortest_path(isp_graph, src, dst, weight='weight')\n",
    "                        for i in range(len(path) - 1):\n",
    "                            link = tuple(sorted([path[i], path[i + 1]]))\n",
    "                            link_frequency[link] += 1\n",
    "                    except nx.NetworkXNoPath:\n",
    "                        continue\n",
    "    \n",
    "    # Calculate total weights for all edges\n",
    "    # Weights are valid regardless of connectivity status!\n",
    "    edge_weights = {}\n",
    "    edge_components = {}\n",
    "    \n",
    "    for u, v in isp_graph.edges():\n",
    "        # Calculate for both directions\n",
    "        for link in [(u, v), (v, u)]:\n",
    "            if isp_id in weights_by_link_by_isp and weights_by_link_by_isp[isp_id]:\n",
    "                weights = weights_by_link_by_isp.get(isp_id, {}).get(link, {})\n",
    "                edge_weights[link] = weights['total']\n",
    "                edge_components[link] = weights\n",
    "            else:\n",
    "                # Fallback only if weights weren't calculated at all\n",
    "                edge_weights[link] = 1.0\n",
    "                edge_components[link] = {\n",
    "                    'isp_usage': 1.0,\n",
    "                    'migration': 0.0,\n",
    "                    'criticality': 0.0,\n",
    "                    'total': 1.0\n",
    "                }\n",
    "    # Create figure\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=figsize)\n",
    "    \n",
    "    # === LEFT PLOT: Full topology with ISP highlighted ===\n",
    "    title = f'ISP {isp_id} - Topologia '\n",
    "    title += '(PARTICIONADA)' if not is_connected else 'com Pesos'\n",
    "    ax1.set_title(title, fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Layout using FULL topology for consistent positioning\n",
    "    # Using seed=7 to match standard project visualizations\n",
    "    pos = nx.spring_layout(topology_graph, seed=7)\n",
    "    \n",
    "    # Get all nodes except disaster\n",
    "    all_nodes = list(topology_graph.nodes())\n",
    "    background_nodes = [n for n in all_nodes if n not in isp_nodes_set and n != disaster_node]\n",
    "    \n",
    "    # STEP 1: Draw background topology (all nodes and edges in gray)\n",
    "    # Draw background nodes\n",
    "    if background_nodes:\n",
    "        nx.draw_networkx_nodes(\n",
    "            topology_graph, pos,\n",
    "            nodelist=background_nodes,\n",
    "            node_color='lightgray',\n",
    "            node_size=500,\n",
    "            edgecolors='gray',\n",
    "            linewidths=1,\n",
    "            alpha=0.3,\n",
    "            ax=ax1\n",
    "        )\n",
    "    \n",
    "    # Draw all edges in light gray\n",
    "    all_edges = list(topology_graph.edges())\n",
    "    nx.draw_networkx_edges(\n",
    "        topology_graph, pos,\n",
    "        edgelist=all_edges,\n",
    "        edge_color='lightgray',\n",
    "        width=1,\n",
    "        alpha=0.3,\n",
    "        ax=ax1\n",
    "    )\n",
    "    \n",
    "    # STEP 2: Draw ISP nodes and edges (highlighted)\n",
    "    isp_nodes_list = list(isp_nodes_active)\n",
    "    \n",
    "    # Draw ISP nodes\n",
    "    if isp_nodes_list:\n",
    "        nx.draw_networkx_nodes(\n",
    "            topology_graph, pos,\n",
    "            nodelist=isp_nodes_list,\n",
    "            node_color='lightblue',\n",
    "            node_size=800,\n",
    "            edgecolors='black',\n",
    "            linewidths=2,\n",
    "            ax=ax1\n",
    "        )\n",
    "    if isp.datacenter.destination:\n",
    "        nx.draw_networkx_nodes(\n",
    "            topology_graph, pos,\n",
    "            nodelist=[isp.datacenter.destination],\n",
    "            node_color='lightgreen',\n",
    "            node_size=800,\n",
    "            edgecolors='black',\n",
    "            linewidths=2,\n",
    "            ax=ax1\n",
    "        )\n",
    "    # STEP 3: Draw disaster node if in ISP\n",
    "    if disaster_node in isp_nodes_set:\n",
    "        nx.draw_networkx_nodes(\n",
    "            topology_graph, pos,\n",
    "            nodelist=[disaster_node],\n",
    "            node_color='red',\n",
    "            node_size=1000,\n",
    "            node_shape='X',\n",
    "            edgecolors='darkred',\n",
    "            linewidths=3,\n",
    "            ax=ax1,\n",
    "            label='Nó do Desastre'\n",
    "        )\n",
    "    \n",
    "    # Draw all node labels\n",
    "    nx.draw_networkx_labels(\n",
    "        topology_graph, pos,\n",
    "        font_size=9,\n",
    "        font_weight='bold',\n",
    "        ax=ax1\n",
    "    )\n",
    "    \n",
    "    # STEP 4: Draw ISP edges with colors and thickness\n",
    "    edges = list(isp_graph.edges())\n",
    "    weights_list = [edge_weights.get((u, v), 1.0) for u, v in edges]\n",
    "    \n",
    "    # Calculate widths based on frequency\n",
    "    max_freq = max(link_frequency.values()) if link_frequency else 1\n",
    "    widths = []\n",
    "    for u, v in edges:\n",
    "        link = tuple(sorted([u, v]))\n",
    "        freq = link_frequency.get(link, 0)\n",
    "        # Width range: 2 to 8\n",
    "        width = 2 + (freq / max_freq) * 6 if max_freq > 0 else 3\n",
    "        widths.append(width)\n",
    "    \n",
    "    # Normalize weights for color mapping (1.0 to max)\n",
    "    vmin = 1.0\n",
    "    vmax = max(weights_list) if weights_list else 1.8\n",
    "    \n",
    "    # Draw ISP edges on full topology (with colors even if partitioned)\n",
    "    nx.draw_networkx_edges(\n",
    "        topology_graph, pos,\n",
    "        edgelist=edges,\n",
    "        edge_color=weights_list,\n",
    "        edge_cmap=plt.cm.RdYlGn_r,\n",
    "        edge_vmin=vmin,\n",
    "        edge_vmax=vmax,\n",
    "        width=widths,\n",
    "        alpha=0.8,\n",
    "        ax=ax1\n",
    "    )\n",
    "    \n",
    "    # Add colorbar\n",
    "    sm = plt.cm.ScalarMappable(\n",
    "        cmap=plt.cm.RdYlGn_r, \n",
    "        norm=plt.Normalize(vmin=vmin, vmax=vmax)\n",
    "    )\n",
    "    sm.set_array([])\n",
    "    cbar = plt.colorbar(sm, ax=ax1, fraction=0.046, pad=0.04)\n",
    "    cbar.set_label('Peso Total', rotation=270, labelpad=20, fontsize=12)\n",
    "    \n",
    "    # Add partition warning if applicable\n",
    "    if not is_connected:\n",
    "        ax1.text(0.5, 0.05, f'[!] Rede particionada em {len(components)} componentes', \n",
    "                transform=ax1.transAxes,\n",
    "                ha='center', fontsize=11, color='red', fontweight='bold',\n",
    "                bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.7))\n",
    "    \n",
    "    ax1.axis('off')\n",
    "    if disaster_node in isp_nodes_set:\n",
    "        ax1.legend(loc='upper left', fontsize=10)\n",
    "    \n",
    "    # === RIGHT PLOT: Weight breakdown (always show if we have weights) ===\n",
    "    if edge_weights:\n",
    "        title = f'ISP {isp_id} - Decomposição dos 10 Links Mais Pesados'\n",
    "        if not is_connected:\n",
    "            title += f' ({len(components)} Componentes)'\n",
    "        ax2.set_title(title, fontsize=16, fontweight='bold')\n",
    "        \n",
    "        # Get top 10 links by weight\n",
    "        sorted_edges = sorted(edge_weights.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "        if sorted_edges:\n",
    "            link_names = [f\"{u}→{v}\" for (u, v), _ in sorted_edges]\n",
    "            \n",
    "            isp_usage_vals = [edge_components[link]['isp_usage'] for link, _ in sorted_edges]\n",
    "            migration_vals = [edge_components[link]['migration'] for link, _ in sorted_edges]\n",
    "            criticality_vals = [edge_components[link]['criticality'] for link, _ in sorted_edges]\n",
    "            \n",
    "            x = range(len(link_names))\n",
    "            bar_height = 0.6\n",
    "            \n",
    "            # Stacked bar chart - build cumulative positions\n",
    "            base_vals = [1.0] * len(x)\n",
    "            \n",
    "            # Calculate cumulative left positions for stacking\n",
    "            left_isp = base_vals\n",
    "            left_migration = [base + isp for base, isp in zip(base_vals, isp_usage_vals)]\n",
    "            left_criticality = [base + isp + mig for base, isp, mig in zip(base_vals, isp_usage_vals, migration_vals)]\n",
    "            \n",
    "            # Draw stacked bars (now bars are actually stacked properly)\n",
    "            p1 = ax2.barh(x, base_vals, bar_height, label='Base (1.0)', color='lightgray', edgecolor='black', linewidth=0.5)\n",
    "            p2 = ax2.barh(x, isp_usage_vals, bar_height, left=left_isp, \n",
    "                         label='ISP Usage', color='skyblue', edgecolor='black', linewidth=0.5)\n",
    "            p3 = ax2.barh(x, migration_vals, bar_height, left=left_migration,\n",
    "                         label='Migration', color='lightgreen', edgecolor='black', linewidth=0.5)\n",
    "            p4 = ax2.barh(x, criticality_vals, bar_height, left=left_criticality,\n",
    "                         label='Criticality', color='salmon', edgecolor='black', linewidth=0.5)\n",
    "            \n",
    "            ax2.set_yticks(x)\n",
    "            ax2.set_yticklabels(link_names, fontsize=10)\n",
    "            ax2.set_xlabel('Peso Total', fontsize=12, fontweight='bold')\n",
    "            ax2.set_ylabel('Link', fontsize=12, fontweight='bold')\n",
    "            vmax = max(w for w in edge_weights.values())\n",
    "            ax2.set_xlim(0.9, vmax * 1.05)\n",
    "            ax2.legend(loc='lower right', fontsize=10, framealpha=0.9)\n",
    "            ax2.grid(axis='x', alpha=0.3, linestyle='--')\n",
    "            \n",
    "            # Add total weight values at the end of bars\n",
    "            for i, (link, total_weight) in enumerate(sorted_edges):\n",
    "                ax2.text(total_weight + 0.02, i, f'{total_weight:.2f}', \n",
    "                        va='center', fontsize=9, fontweight='bold')\n",
    "    else:\n",
    "        # Fallback: No weights available\n",
    "        ax2.set_title(f'ISP {isp_id} - Sem Dados de Pesos', \n",
    "                      fontsize=16, fontweight='bold')\n",
    "        ax2.text(0.5, 0.5, \n",
    "                'Pesos não foram calculados para esta ISP',\n",
    "                ha='center', va='center', fontsize=12,\n",
    "                transform=ax2.transAxes)\n",
    "        ax2.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print statistics\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"ESTATÍSTICAS - ISP {isp_id}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Nós da ISP (total): {len(isp_nodes_set)}\")\n",
    "    print(f\"Nós após remover desastre: {len(isp_nodes_active)}\")\n",
    "    print(f\"Links da ISP: {isp_graph.number_of_edges()}\")\n",
    "    print(f\"Conectada: {'Sim' if is_connected else 'Não'}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Creating Interactive ISP Visualization GUI...\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c808616a14d54993bf79f8df13daaa0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h3>🔍 ISP Topology Viewer with Weight Analysis</h3><p>Select an ISP from the dropdo…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "def create_isp_visualization_gui(\n",
    "    lista_de_isps,\n",
    "    topology_graph,\n",
    "    disaster_node,\n",
    "    weights_by_link_by_isp\n",
    "):\n",
    "    \"\"\"\n",
    "    Create an interactive GUI to visualize ISP topologies with weights.\n",
    "    \n",
    "    Args:\n",
    "        lista_de_isps: List of ISP objects\n",
    "        topology_graph: Main topology graph\n",
    "        disaster_node: Node affected by disaster\n",
    "        weights_by_link_by_isp: Computed weights for all ISPs\n",
    "    \"\"\"\n",
    "    # Create output widget to capture the plot\n",
    "    output = widgets.Output()\n",
    "    \n",
    "    # Create dropdown with ISP options\n",
    "    isp_options = [(f\"ISP {isp.isp_id}\", isp.isp_id) for isp in lista_de_isps]\n",
    "    dropdown = widgets.Dropdown(\n",
    "        options=isp_options,\n",
    "        value=lista_de_isps[0].isp_id,\n",
    "        description='Select ISP:',\n",
    "        style={'description_width': 'initial'},\n",
    "        layout=widgets.Layout(width='300px')\n",
    "    )\n",
    "    \n",
    "    # Create info label\n",
    "    info_label = widgets.HTML(\n",
    "        value=\"<h3>🔍 ISP Topology Viewer with Weight Analysis</h3>\"\n",
    "              \"<p>Select an ISP from the dropdown to view its topology and weight decomposition.</p>\",\n",
    "        layout=widgets.Layout(margin='0 0 20px 0')\n",
    "    )\n",
    "    \n",
    "    # Function to update visualization when dropdown changes\n",
    "    def on_isp_change(change):\n",
    "        with output:\n",
    "            clear_output(wait=True)\n",
    "            \n",
    "            isp_id = change['new']\n",
    "            isp = lista_de_isps[isp_id]\n",
    "            \n",
    "            print(f\"📊 Loading visualization for ISP {isp_id}...\")\n",
    "            print(\"=\"*80)\n",
    "            \n",
    "            # Call the visualization function\n",
    "            visualize_isp_topology_with_weights(\n",
    "                isp,\n",
    "                topology_graph,\n",
    "                disaster_node,\n",
    "                isp_id,\n",
    "                weights_by_link_by_isp\n",
    "            )\n",
    "    \n",
    "    # Attach the callback to dropdown\n",
    "    dropdown.observe(on_isp_change, names='value')\n",
    "    \n",
    "    # Create the layout\n",
    "    gui = widgets.VBox([\n",
    "        info_label,\n",
    "        dropdown,\n",
    "        output\n",
    "    ])\n",
    "    \n",
    "    # Display initial visualization\n",
    "    with output:\n",
    "        isp_id = dropdown.value\n",
    "        isp = lista_de_isps[isp_id]\n",
    "        \n",
    "        print(f\"📊 Loading visualization for ISP {isp_id}...\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        visualize_isp_topology_with_weights(\n",
    "            isp,\n",
    "            topology_graph,\n",
    "            disaster_node,\n",
    "            isp_id,\n",
    "            weights_by_link_by_isp\n",
    "        )\n",
    "    \n",
    "    return gui\n",
    "\n",
    "\n",
    "# Create and display the GUI\n",
    "print(\"🚀 Creating Interactive ISP Visualization GUI...\\n\")\n",
    "gui = create_isp_visualization_gui(\n",
    "    lista_de_isps,\n",
    "    topology.topology,\n",
    "    disaster_node,\n",
    "    weights_by_link_by_isp\n",
    ")\n",
    "\n",
    "display(gui)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
