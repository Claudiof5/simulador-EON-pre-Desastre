{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Simulador import Simulador\n",
    "from Registrador import Registrador\n",
    "from simpy import Environment\n",
    "import networkx as nx\n",
    "from Cenario.GeradorDeCenarios import GeradorDeCenarios, Cenario\n",
    "from Variaveis import *\n",
    "from Roteamento.Roteamento import Roteamento\n",
    "from Roteamento.RoteamentoBestFit import RoteamentoBestFit\n",
    "from Roteamento.Roteamento_evitando_nodes_pre_desastre import Roteamento_evitando_nodes_pre_desastre\n",
    "from Roteamento.RoteamentoBestFit_evitando_nodes_pre_desastre import RoteamentoBestFit_evitando_nodes_pre_desastre\n",
    "from Roteamento.RoteamentoBestFit_usando_sliding_windown import RoteamentoBestFit_evitando_nodes_pre_desastre_com_bloqueio\n",
    "from Roteamento.Roteamento_evitando_nodes_pre_desastre_com_bloqueio_artificial import Roteamento_evitando_nodes_pre_desastre_com_bloqueio\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "TOPOLOGY = nx.read_weighted_edgelist( \"../topology/usa\", nodetype=int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encontra bons pontos de desastre para topologia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __find_balanced_min_cut(graph: nx.Graph):\n",
    "\n",
    "    def __recover_partition_data(partition, graph: nx.Graph):\n",
    "        reachable, non_reachable = partition\n",
    "        cut_edges = [(u, v) for u, v in graph.edges() if (u in reachable and v in non_reachable) or (v in reachable and u in non_reachable)]\n",
    "        return {\n",
    "            \"partition_1\": reachable if len(reachable) < len(non_reachable) else non_reachable,\n",
    "            \"partition_2\": non_reachable if len(reachable) < len(non_reachable) else reachable,\n",
    "            \"cut_edges\": cut_edges\n",
    "        }\n",
    "    # Set default capacities if missing\n",
    "    for u, v in graph.edges():\n",
    "        if 'capacity' not in graph[u][v]:\n",
    "            graph[u][v]['capacity'] = 1  # Default capacity for unweighted edges\n",
    "\n",
    "    best_cut_value = float(\"inf\")\n",
    "    best_size_difference = float(\"inf\")\n",
    "    best_partitions_set = set()  # To store unique partitions\n",
    "    # Iterate over all pairs of nodes as source (s) and sink (t)\n",
    "    for s in graph.nodes():\n",
    "        for t in graph.nodes():\n",
    "            if s == t:\n",
    "                continue\n",
    "\n",
    "            # Skip pairs where there is no path\n",
    "            if not nx.has_path(graph, s, t):\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                # Calculate the minimum cut between s and t\n",
    "                cut_value, partition = nx.minimum_cut(graph, s, t)\n",
    "                (reachable, non_reachable) = partition\n",
    "\n",
    "                # Ensure balance between the two partitions\n",
    "                size_difference = abs(len(reachable) - len(non_reachable))\n",
    "\n",
    "                # Create a frozenset to compare unique partitions\n",
    "                partition_frozen = frozenset((frozenset(reachable), frozenset(non_reachable)))\n",
    "\n",
    "                if (cut_value < best_cut_value or\n",
    "                    ( cut_value == best_cut_value and size_difference < best_size_difference )):\n",
    "                    best_cut_value = cut_value\n",
    "                    best_size_difference = size_difference\n",
    "                    best_partitions_set = {partition_frozen}  # Reset to only this partition\n",
    "\n",
    "                elif cut_value == best_cut_value and size_difference == best_size_difference:\n",
    "                    best_partitions_set.add(partition_frozen)  # Add only if unique\n",
    "\n",
    "            except nx.NetworkXUnbounded:\n",
    "                print(f\"Unbounded flow for source {s} and sink {t}, skipping this pair.\")\n",
    "\n",
    "    # Create list of partition data\n",
    "    \n",
    "    results = []\n",
    "    for partition in best_partitions_set:\n",
    "        partition_data = __recover_partition_data(partition, graph)\n",
    "        results.append(partition_data)\n",
    "\n",
    "    return {\n",
    "        \"partitions\": results,\n",
    "        \"min_cut_value\": best_cut_value,\n",
    "        \"min_size_difference\": best_size_difference\n",
    "    }\n",
    " \n",
    "def __remove_node_from_graph( graph: nx.Graph, nodes: list[int]):\n",
    "    graph = graph.copy()\n",
    "    graph.remove_nodes_from(nodes)\n",
    "    return graph\n",
    "\n",
    "def __find_balanced_min_cut_for_all_nodes():\n",
    "    original_graph = TOPOLOGY\n",
    "    dados = []\n",
    "    for node in original_graph.nodes():\n",
    "        copy_graph = __remove_node_from_graph(original_graph, [node])\n",
    "        dado = __find_balanced_min_cut(copy_graph)\n",
    "        dado[\"node\"] = node\n",
    "        dados.append(dado)\n",
    "    dados.sort(key=lambda x: x['min_size_difference'])\n",
    "    return dados\n",
    "\n",
    "def print_balanced_min_cut_for_all_nodes():\n",
    "    for dado in __find_balanced_min_cut_for_all_nodes():\n",
    "        if dado['min_size_difference'] == 21:\n",
    "            continue\n",
    "        print( f\"Node {dado['node']} removido, corte minimo = {dado['min_cut_value']}, diferença de tamanho minima = {dado['min_size_difference']}\")\n",
    "        for particao in dado[\"partitions\"]:\n",
    "            print( f\"Arestas de corte {particao[\"cut_edges\"]},\\n Partição 1: {list(particao['partition_1'])}, Partição 2: {list(particao['partition_2'])}\")\n",
    "        print(\"\\n\\n\")\n",
    "\n",
    "\n",
    "__info_pior_desastre = __find_balanced_min_cut_for_all_nodes()[0]\n",
    "NODE_DESASTRE = __info_pior_desastre[\"node\"]\n",
    "COMPONENTE_1 = __info_pior_desastre[\"partitions\"][0][\"partition_1\"]\n",
    "COMPONENTE_2 = __info_pior_desastre[\"partitions\"][0][\"partition_2\"]\n",
    "CRITICAL_EDGES = __info_pior_desastre[\"partitions\"][0][\"cut_edges\"]\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cria cenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "criar_e_rodar_novo_cenario = False\n",
    "is_logger_active = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Environment()\n",
    "\n",
    "\n",
    "if criar_e_rodar_novo_cenario:\n",
    "    roteamentos_a_serem_testados = [\n",
    "        Roteamento_evitando_nodes_pre_desastre_com_bloqueio,\n",
    "        Roteamento_evitando_nodes_pre_desastre\n",
    "        ]\n",
    "    \n",
    "    cenario1, cenario2 = GeradorDeCenarios.gerar_cenarios(\n",
    "        TOPOLOGY, retorna_lista_de_requisicoes=True, numero_de_requisicoes=NUMERO_DE_REQUISICOES, \n",
    "        lista_de_roteamentos_de_desastre = roteamentos_a_serem_testados, )\n",
    "\n",
    "    #salvar cenarios\n",
    "    with open('cenario/cenarios/cenario1.pkl', 'wb') as file:\n",
    "        pickle.dump(cenario1, file) \n",
    "\n",
    "    with open('cenario/cenarios/cenario2.pkl', 'wb') as file:\n",
    "        pickle.dump(cenario2, file) \n",
    "\n",
    "#carregar cenarios\n",
    "cenario1: Cenario = pickle.load(open('cenario/cenarios/cenario1.pkl', 'rb'))\n",
    "cenario2: Cenario = pickle.load(open('cenario/cenarios/cenario2.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Roda cenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0k/s6m7zdf54hgbxnlcw473w_1h0000gq/T/ipykernel_88184/3686860743.py:23: DtypeWarning: Columns (10,19) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  dataframe1: pd.DataFrame = pd.read_csv(\"_out/resultados/df_cenario1.csv\")\n",
      "/var/folders/0k/s6m7zdf54hgbxnlcw473w_1h0000gq/T/ipykernel_88184/3686860743.py:24: DtypeWarning: Columns (19) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  dataframe2: pd.DataFrame = pd.read_csv(\"_out/resultados/df_cenario2.csv\")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if criar_e_rodar_novo_cenario:\n",
    "    #roda cenario1\n",
    "    Registrador.reseta_registrador()\n",
    "    env = Environment()\n",
    "    simulador :Simulador  = Simulador(env=env, topology=TOPOLOGY, status_logger=is_logger_active, cenario=cenario1)\n",
    "    simulador.run()\n",
    "    dataframe_cenario1 = simulador.salvar_dataframe(\"_out/resultados/df_cenario1\")\n",
    "    dataframe_sliding_window = Registrador.cria_dataframe_janela_deslizante(\"_out/dataset1\")\n",
    "    dataframe_media_disponibilidade_extra_componente = Registrador.cria_dataframe_media_taxa_de_disponibilidade_extra_componente(\"_out/dataset1\")\n",
    "    dataframe_bloqueio_artificial = Registrador.cria_dataframe_bloqueio_artificial(\"_out/dataset1\")\n",
    "    \n",
    "    #roda cenario2\n",
    "    env = Environment()\n",
    "    Registrador.reseta_registrador()\n",
    "    simulador :Simulador  = Simulador(env=env, topology=TOPOLOGY, status_logger=is_logger_active, cenario=cenario2)\n",
    "    simulador.run()\n",
    "    dataframe2_sliding_window = Registrador.cria_dataframe_janela_deslizante(\"_out/dataset2\")\n",
    "    dataframe2_media_disponibilidade_extra_componente = Registrador.cria_dataframe_media_taxa_de_disponibilidade_extra_componente(\"_out/dataset2\")\n",
    "    dataframe_cenario2 = simulador.salvar_dataframe(\"_out/resultados/df_cenario2\")\n",
    "    dataframe2_bloqueio_artificial = Registrador.cria_dataframe_bloqueio_artificial(\"_out/dataframe2\")\n",
    "\n",
    "\n",
    "dataframe1: pd.DataFrame = pd.read_csv(\"_out/resultados/df_cenario1.csv\")\n",
    "dataframe2: pd.DataFrame = pd.read_csv(\"_out/resultados/df_cenario2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funções de calculo auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_average_disponibility( dataframe):\n",
    "        \n",
    "    number_of_requests = len(dataframe)\n",
    "    number_of_accepted_requests = len(dataframe[dataframe[\"bloqueada\"] == False ])\n",
    "    return number_of_accepted_requests / number_of_requests\n",
    "\n",
    "def __devolve_dict_distancia_minima_entre_nodes():\n",
    "    dict_nodes_para_distancia_minima_entre_nodes = {}\n",
    "    dict_distancia_minima_entre_nodes_para_nodes = defaultdict(list)\n",
    "    for node1 in cenario1.topology.topology:\n",
    "        for node2 in cenario1.topology.topology:\n",
    "            if node1 == node2:\n",
    "                continue\n",
    "\n",
    "            menor_caminho = cenario1.topology.caminhos_mais_curtos_entre_links[node1][node2][0][\"caminho\"]\n",
    "            tamanho_menor_caminho = len(menor_caminho) - 1\n",
    "            dict_nodes_para_distancia_minima_entre_nodes[(node1, node2)] = tamanho_menor_caminho\n",
    "            dict_distancia_minima_entre_nodes_para_nodes[tamanho_menor_caminho].append((node1, node2))\n",
    "\n",
    "    return dict_distancia_minima_entre_nodes_para_nodes\n",
    "\n",
    "def __devolve_distancia_minima_entre_nodes_extra_componente( ):\n",
    "\n",
    "    distancia_minima_entre_nodes_para_nodes_extra_componente = defaultdict(list)\n",
    "    for distancia, links in __devolve_dict_distancia_minima_entre_nodes().items():\n",
    "        for link in links:\n",
    "            if (link[0] in COMPONENTE_1 and link[1] in COMPONENTE_2) or (link[0] in COMPONENTE_2 and link[1] in COMPONENTE_1):\n",
    "                distancia_minima_entre_nodes_para_nodes_extra_componente[distancia].append(link)\n",
    "    return distancia_minima_entre_nodes_para_nodes_extra_componente\n",
    "\n",
    "def __retorna_dict_numero_de_vizinhos_por_node():\n",
    "    dict_node_numero_de_vizinhos = {}\n",
    "    dict_numero_de_vizinhos_por_nodes = defaultdict(list)\n",
    "    for node in cenario1.topology.topology:\n",
    "        numero_de_vizinhos = len(list(cenario1.topology.topology.neighbors(node)))\n",
    "        dict_node_numero_de_vizinhos[node] = numero_de_vizinhos\n",
    "        dict_numero_de_vizinhos_por_nodes[numero_de_vizinhos].append(node)\n",
    "    return dict_numero_de_vizinhos_por_nodes\n",
    "\n",
    "def calculate_average_distance_between_groups_of_nodes( grupo_de_nodes_1, grupo_de_nodes_2):\n",
    "    distancia_media_entre_grupos = 0\n",
    "    dict_nodes_para_distancia_minima_entre_nodes = __devolve_dict_distancia_minima_entre_nodes()\n",
    "\n",
    "    for node1 in grupo_de_nodes_1:\n",
    "        for node2 in grupo_de_nodes_2:\n",
    "            distancia_media_entre_grupos += dict_nodes_para_distancia_minima_entre_nodes[(node1, node2)]\n",
    "    distancia_media_entre_grupos /= len(grupo_de_nodes_1) * len(grupo_de_nodes_2)\n",
    "    return distancia_media_entre_grupos\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funcões de manipulação do dataframe de requisições"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def devolve_extra_component_para_node(node, dataframe):\n",
    "    componente_oposto = COMPONENTE_1 if node in COMPONENTE_2 else COMPONENTE_2\n",
    "    return dataframe[\n",
    "        (dataframe['src'] == node) & (dataframe[\"dst\"].isin(componente_oposto))\n",
    "        ]\n",
    "\n",
    "def devolve_extra_component_para_link(link, dataframe):\n",
    "    return dataframe[\n",
    "        (dataframe['src'] == link[0]) & (dataframe[\"dst\"] == link[1])\n",
    "        ]\n",
    "\n",
    "def devolve_dataframe_desastre(dataframe):\n",
    "    \n",
    "    first_migration_time = min([ (isp.datacenter.tempo_de_reacao, isp.id) for isp in cenario1.lista_de_ISPs])[0]\n",
    "    desaster_end_time = cenario1.desastre.start + cenario1.desastre.duration\n",
    "\n",
    "    return dataframe[\n",
    "        (dataframe[\"tempo_criacao\"] >= first_migration_time) & \n",
    "        (dataframe[\"tempo_criacao\"] <= desaster_end_time)\n",
    "    ]\n",
    "\n",
    "def devolve_dataframe_antes_do_desastre(dataframe):\n",
    "    \n",
    "    first_migration_time = min([ (isp.datacenter.tempo_de_reacao, isp.id) for isp in cenario1.lista_de_ISPs])[0]\n",
    "    return dataframe[\n",
    "        (dataframe[\"tempo_criacao\"] <= first_migration_time)\n",
    "    ]\n",
    "\n",
    "def devolve_dataframe_depois_do_desastre(dataframe):\n",
    "    \n",
    "    desaster_end_time = cenario1.desastre.start + cenario1.desastre.duration\n",
    "\n",
    "    return dataframe[\n",
    "        (dataframe[\"tempo_criacao\"] >= desaster_end_time)\n",
    "    ]\n",
    "\n",
    "def devolve_dataframe_apenas_extra_componente(dataframe):\n",
    "    return dataframe[\n",
    "        (dataframe['src'].isin(COMPONENTE_1)) & (dataframe[\"dst\"].isin(COMPONENTE_2)) | \n",
    "        (dataframe['src'].isin(COMPONENTE_2)) & (dataframe[\"dst\"].isin(COMPONENTE_1))\n",
    "    ]\n",
    "\n",
    "def devolve_dict_de_dataframes_distancia_entre_nodes_para_df(dataframe):\n",
    "    dataframe_desastre = devolve_dataframe_desastre(dataframe)\n",
    "\n",
    "    dataframes_trafego_extra_componente_por_distancia = defaultdict(pd.DataFrame)\n",
    "    for distancia, links in __devolve_distancia_minima_entre_nodes_extra_componente().items():\n",
    "        df = pd.DataFrame()\n",
    "        for link in links:\n",
    "            df = pd.concat([df, devolve_extra_component_para_link(link, dataframe_desastre)])\n",
    "        df = df.sort_values('tempo_criacao')\n",
    "        df.drop_duplicates()\n",
    "        dataframes_trafego_extra_componente_por_distancia[distancia] = df\n",
    "    return dataframes_trafego_extra_componente_por_distancia\n",
    "\n",
    "def devolve_dict_de_dataframes_numero_de_vizinhos_para_df(dataframe: pd.DataFrame):\n",
    "    dict_numero_de_vizinhos_por_nodes = __retorna_dict_numero_de_vizinhos_por_node()\n",
    "    dict_dataframes_numero_de_vizinhos = defaultdict(pd.DataFrame)\n",
    "    for numero_de_vizinhos, nodes in dict_numero_de_vizinhos_por_nodes.items():\n",
    "        filtered_dataframe = dataframe[dataframe.apply(lambda row: row[\"src\"] in nodes, axis=1)]\n",
    "        dict_dataframes_numero_de_vizinhos[numero_de_vizinhos] = filtered_dataframe\n",
    "    return dict_dataframes_numero_de_vizinhos\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funções de retorno de Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retorna_dict_de_delta_disponibilidade_por_node(dataframe: pd.DataFrame):\n",
    "    dict_de_delta_disponibilidade = {}\n",
    "    for node in cenario1.topology.topology.nodes:\n",
    "        if node == 9:\n",
    "            continue\n",
    "        dataframe_antes_do_desastre = devolve_dataframe_antes_do_desastre(dataframe)\n",
    "        dataframe_depois_do_desastre = devolve_dataframe_depois_do_desastre(dataframe)\n",
    "        bloqueio_antes = calculate_average_disponibility(devolve_extra_component_para_node(node, dataframe_antes_do_desastre))\n",
    "        bloqueio_durante = calculate_average_disponibility(devolve_extra_component_para_node(node, dataframe_depois_do_desastre))\n",
    "        bloqueio_antes = 1-bloqueio_antes\n",
    "        bloqueio_durante = 1-bloqueio_durante\n",
    "        dict_de_delta_disponibilidade[node]= { \n",
    "            \"texto\":f\"node {node:02d}, delta disponibilidade {(bloqueio_durante - bloqueio_antes)*100:.4f}%, razão {(bloqueio_durante / bloqueio_antes)*100:.4f}%, \" + \n",
    "              f\"disponibilidade antes {bloqueio_antes*100:.4f}%, disponibilidade durante {bloqueio_durante*100:.4f}%\", \n",
    "            \"delta\":(bloqueio_durante - bloqueio_antes)*100, \"razão\":(bloqueio_durante/ bloqueio_antes)*100, \"disponibilidade_antes\": bloqueio_antes, \"disponibilidade_durante\": bloqueio_durante}\n",
    "    return dict_de_delta_disponibilidade\n",
    "\n",
    "def retorna_dados_delta_e_razão_degradação( dataframe):\n",
    "\n",
    "    dict_delta_bloqueios = retorna_dict_de_delta_disponibilidade_por_node(dataframe)\n",
    "    delta_degradação = [ value[\"delta\"] for value in dict_delta_bloqueios.values()]\n",
    "    razao_degradação = [ value[\"razão\"] for value in dict_delta_bloqueios.values()]\n",
    "    min_delta = min(delta_degradação)\n",
    "    max_delta = max(delta_degradação)\n",
    "    min_razao = min(razao_degradação)\n",
    "    max_razao = max(razao_degradação)\n",
    "    variancia_delta = np.var(delta_degradação)\n",
    "    media_delta = np.mean(delta_degradação)\n",
    "    variancia_razao = np.var(razao_degradação)\n",
    "    media_razao = np.mean(razao_degradação)\n",
    "    return { \"delta\": { \"max\": max_delta, \"min\": min_delta, \"variancia\": variancia_delta, \"media\": media_delta},\n",
    "            \"razão\": { \"max\": max_razao, \"min\": min_razao, \"variancia\": variancia_razao, \"media\": media_razao}}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funções de mostrar dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_average_disponibility_during_disaster(dataframe1, dataframe2):\n",
    "\n",
    "    dataframe1_desastre_cross_component = devolve_dataframe_apenas_extra_componente(devolve_dataframe_desastre(dataframe1))\n",
    "\n",
    "    dataframe2_desastre_cross_component = devolve_dataframe_apenas_extra_componente(devolve_dataframe_desastre(dataframe2))\n",
    "\n",
    "    disponibilidade_media_dataframe1 = calculate_average_disponibility(dataframe1_desastre_cross_component)\n",
    "    disponibilidade_media_dataframe2 = calculate_average_disponibility(dataframe2_desastre_cross_component)\n",
    "\n",
    "    print(f\"Disponibilidade media durante o desastre dataframe1: {disponibilidade_media_dataframe1*100:.2f}%\")\n",
    "    print(f\"Disponibilidade media durante o desastre dataframe2: {disponibilidade_media_dataframe2*100:.2f}%\")\n",
    "\n",
    "    for node in range(1, 25):\n",
    "        if node == 9:\n",
    "            continue\n",
    "        disponibilidade_media_node_dataframe1 = calculate_average_disponibility(devolve_extra_component_para_node(node, dataframe1_desastre_cross_component))\n",
    "        disponibilidade_media_node_dataframe2 = calculate_average_disponibility(devolve_extra_component_para_node(node, dataframe2_desastre_cross_component))\n",
    "\n",
    "        print(f\"----------node {node} diferença de disponibilidade {(disponibilidade_media_node_dataframe1 - disponibilidade_media_node_dataframe2)*100:.2f}%------------------\")\n",
    "        print(f\"Disponibilidade media durante o desastre node {node} dataframe1: {disponibilidade_media_node_dataframe1*100:.2f}%\")\n",
    "        print(f\"Disponibilidade media durante o desastre node {node} dataframe2: {disponibilidade_media_node_dataframe2*100:.2f}%\")\n",
    "    \n",
    "def print_average_disponibility_during_disaster_por_distancia(dataframe1, dataframe2):\n",
    "    dataframe1_desastre_cross_component_by_distance = devolve_dict_de_dataframes_distancia_entre_nodes_para_df(dataframe1)\n",
    "    dataframe2_desastre_cross_component_by_distance = devolve_dict_de_dataframes_distancia_entre_nodes_para_df(dataframe2)\n",
    "\n",
    "    for distance in dataframe1_desastre_cross_component_by_distance.keys():\n",
    "        \n",
    "        disponibilidade_media_disastre_dataframe1 = calculate_average_disponibility(dataframe1_desastre_cross_component_by_distance[distance])\n",
    "        disponibilidade_media_disastre_dataframe2 = calculate_average_disponibility(dataframe2_desastre_cross_component_by_distance[distance])\n",
    "\n",
    "        print(f\"----------distancia {distance} diferença de disponibilidade {(disponibilidade_media_disastre_dataframe1 - disponibilidade_media_disastre_dataframe2)*100:.2f}%------------------\")\n",
    "\n",
    "        print(f\"disponibilidade media do desastre para distancia {distance}: {disponibilidade_media_disastre_dataframe1*100:.2f}%\")\n",
    "        print(f\"disponibilidade media do desastre para distancia {distance}: {disponibilidade_media_disastre_dataframe2*100:.2f}%\")\n",
    "\n",
    "def print_average_distance_between_components():\n",
    "    distancia_media_entre_componentes_1_2 = calculate_average_distance_between_groups_of_nodes(COMPONENTE_1, COMPONENTE_2)\n",
    "    distancia_media_entre_componentes_1_1 = calculate_average_distance_between_groups_of_nodes(COMPONENTE_1, COMPONENTE_1)\n",
    "    distancia_media_entre_componentes_2_2 = calculate_average_distance_between_groups_of_nodes(COMPONENTE_2, COMPONENTE_2)\n",
    "    print(f\"distancia media entre componentes 1 e 2: {distancia_media_entre_componentes_1_2}\")\n",
    "    print(f\"distancia media entre componentes 1 e 1: {distancia_media_entre_componentes_1_1}\")\n",
    "    print(f\"distancia media entre componentes 2 e 2: {distancia_media_entre_componentes_2_2}\")\n",
    "\n",
    "def print_delta_disponibilidade_por_node():\n",
    "    for value in retorna_dict_de_delta_disponibilidade_por_node().values():\n",
    "        print(value[\"texto\"])\n",
    "\n",
    "def print_dados_delta_e_razão_degradação(dataframe):\n",
    "    dados = retorna_dados_delta_e_razão_degradação(dataframe)\n",
    "    max_delta = dados[\"delta\"][\"max\"]\n",
    "    min_delta = dados[\"delta\"][\"min\"]\n",
    "    variancia_delta = dados[\"delta\"][\"variancia\"]\n",
    "    media_delta = dados[\"delta\"][\"media\"]\n",
    "    max_razao = dados[\"razão\"][\"max\"]\n",
    "    min_razao = dados[\"razão\"][\"min\"]\n",
    "    variancia_razao = dados[\"razão\"][\"variancia\"]\n",
    "    media_razao = dados[\"razão\"][\"media\"]\n",
    "    print(f\"maximo delta degradação {max_delta:.4f}%, minimum delta degradação {min_delta:.4f}%, variancia {variancia_delta:.4f}, media {media_delta:.4f}\")\n",
    "    print(f\"maximo razão degradação {max_razao:.4f}%, minimum razão degradação {min_razao:.4f}%, variancia {variancia_razao:.4f}, media {media_razao:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_traffic_dataframes(dataframe1, dataframe2):\n",
    "    \"\"\"\n",
    "    Creates separate dataframes for different types of traffic\n",
    "    \"\"\"\n",
    "    #cross component traffic\n",
    "    cross_component_df = devolve_dataframe_apenas_extra_componente(dataframe1)\n",
    "\n",
    "    cross_component_df2 = devolve_dataframe_apenas_extra_componente(dataframe2)\n",
    "\n",
    "    cross_component_node_10_df1 = devolve_extra_component_para_node(10, dataframe1)\n",
    "\n",
    "    cross_component_node_10_df2 = devolve_extra_component_para_node(10, dataframe2)\n",
    "\n",
    "\n",
    "    cross_component_node_24_df1 = devolve_extra_component_para_node(24, dataframe1)\n",
    "\n",
    "    cross_component_node_24_df2 = devolve_extra_component_para_node(24, dataframe2)\n",
    "\n",
    "    return {\n",
    "        \"Cross Component df1\":cross_component_df,\n",
    "        \"Cross Component df2\":cross_component_df2,\n",
    "        \"Cross Component Node 10\":cross_component_node_10_df1,\n",
    "        \"Cross Component Node 10 2\":cross_component_node_10_df2,\n",
    "        \"Cross Component Node 24\":cross_component_node_24_df1,\n",
    "        \"Cross Component Node 24 2\":cross_component_node_24_df2,\n",
    "         #   \"complete Dataset\":original_df\n",
    "        }\n",
    "    \n",
    "dataframes_dict = create_traffic_dataframes(dataframe1, dataframe2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funções de plotagem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plota_migracao_e_desastre():\n",
    "    #plota linhas verticais de inicio de migração\n",
    "    tempos_de_inicio_migracao_ISP = [ (isp.datacenter.tempo_de_reacao, isp.id) for isp in cenario1.lista_de_ISPs] \n",
    "\n",
    "    for x in tempos_de_inicio_migracao_ISP:\n",
    "        plt.axvline(x=x[0], color='red', linestyle='--', linewidth=1)\n",
    "\n",
    "    #plota inicio e fim do desastre\n",
    "    inicio_desastre = cenario1.desastre.start\n",
    "    fim_desastre = cenario1.desastre.start + cenario1.desastre.duration\n",
    "\n",
    "    plt.axvline(x=inicio_desastre, color='black', linestyle='--', linewidth=1)\n",
    "    plt.axvline(x=fim_desastre, color='black', linestyle='--', linewidth=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_accumulated_blocked_requests(dataframes_dict):\n",
    "    plt.figure(figsize=(18, 9), dpi=300)    \n",
    "    for key, dataframe in dataframes_dict.items():\n",
    "        df = dataframe\n",
    "        name = key\n",
    "        \n",
    "        # Total blocked requests\n",
    "        df_blocked = df[df[\"bloqueada\"] == True]\n",
    "        \n",
    "        df_accumulated = df_blocked.groupby('tempo_criacao').size().cumsum()\n",
    "        plt.plot(df_accumulated.index, df_accumulated.values, \n",
    "                label=f\"{name} - Total Blocked\")\n",
    "\n",
    "    \n",
    "    # Plot migration and disaster lines\n",
    "    plota_migracao_e_desastre()\n",
    "    \n",
    "    plt.xlabel('Segundos')\n",
    "    plt.ylabel('Requisições Bloqueadas Acumuladas')\n",
    "    plt.title('Requisições Bloqueadas Acumuladas por Segundo')\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def plot_number_of_blocked_by_bucket(dataframe_dict, bucket_size=10):\n",
    "    plt.figure(figsize=(18, 9), dpi=300)    \n",
    "    for name, dataframe in dataframe_dict.items():\n",
    "        segundos = (dataframe['tempo_criacao'] // bucket_size) * bucket_size\n",
    "        bloqueadas_por_segundo1 = dataframe[dataframe[\"bloqueada\"] == True].groupby(segundos).size()\n",
    "        plt.plot(bloqueadas_por_segundo1.index, bloqueadas_por_segundo1.values, label=f\" {name} bloqueadas por {bucket_size} segundos\")\n",
    "    plota_migracao_e_desastre()\n",
    "\n",
    "    plt.xlabel('Segundos')\n",
    "    plt.ylabel('Requisições Bloqueadas Acumuladas')\n",
    "    plt.title('Requisições Bloqueadas Acumuladas por Segundo')\n",
    "\n",
    "    plt.legend(loc = 'upper left')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def plot_blocked_ratio_by_bucket(dataframe_dict, bucket_size=30):\n",
    "    plt.figure(figsize=(18, 9), dpi=300)    \n",
    "\n",
    "    for name, dataframe in dataframe_dict.items():\n",
    "\n",
    "        segundos = (dataframe['tempo_criacao'] // bucket_size) * bucket_size\n",
    "        total_requisicoes_por_segundo1 = dataframe.groupby(segundos).size()\n",
    "        bloqueadas_por_segundo1 = dataframe[dataframe[\"bloqueada\"] == True].groupby(segundos).size()\n",
    "        bloqueadas_por_segundo1 = bloqueadas_por_segundo1.reindex(total_requisicoes_por_segundo1.index, fill_value=0)\n",
    "        proporcao_bloqueadas1 = bloqueadas_por_segundo1 / total_requisicoes_por_segundo1\n",
    "        #plt.plot(proporcao_bloqueadas1.index, proporcao_bloqueadas1.values)\n",
    "        plt.plot(proporcao_bloqueadas1.index, proporcao_bloqueadas1.values, label=f\"{name} taxa de bloqueio por {bucket_size} segundos\")\n",
    "    plota_migracao_e_desastre()\n",
    "\n",
    "\n",
    "    plt.xlabel('Segundos')\n",
    "    plt.ylabel('Proporção de Requisições Bloqueadas')\n",
    "    plt.title(f'Proporção de Requisições Bloqueadas por {bucket_size} Segundo(s) ')\n",
    "    plt.grid(True)\n",
    "    plt.legend(loc = 'upper left')\n",
    "    plt.show()  \n",
    "\n",
    "def plot_blocked_ratio_sliding_window(dataframes_dict, window_size=10):\n",
    "    \"\"\"\n",
    "    Plot blocked ratio using a sliding window with improved smoothing\n",
    "    \n",
    "    Args:\n",
    "        dataframes_dict (dict): Dictionary with names as keys and dataframes as values\n",
    "        window_size (int): Size of the sliding window in seconds (default: 10)\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(18, 9), dpi=300)\n",
    "    \n",
    "    for name, df in dataframes_dict.items():\n",
    "        # Sort by time to ensure correct window calculations\n",
    "        df = df.sort_values('tempo_criacao')\n",
    "        \n",
    "        # Create a more granular time series (e.g., 0.1 second intervals for smoother curve)\n",
    "        all_times = np.arange(df['tempo_criacao'].min(), \n",
    "                            df['tempo_criacao'].max(), \n",
    "                            0.1)  # 0.1 second steps\n",
    "        \n",
    "        # Calculate blocking ratio using rolling window\n",
    "        ratios = []\n",
    "        for time in all_times:\n",
    "            window_df = df[(df['tempo_criacao'] >= time - window_size/2) & \n",
    "                         (df['tempo_criacao'] < time + window_size/2)]\n",
    "            \n",
    "            if len(window_df) >= 5:  # Minimum sample size threshold\n",
    "                ratio = window_df['bloqueada'].mean()  # Using mean is equivalent to blocked/total\n",
    "            else:\n",
    "                # Use the last valid ratio or 0 if it's the start\n",
    "                ratio = ratios[-1] if ratios else 0\n",
    "                \n",
    "            ratios.append(ratio)\n",
    "        \n",
    "        # Optional: Apply additional smoothing\n",
    "        ratios = pd.Series(ratios).rolling(window=5, min_periods=1, center=True).mean()\n",
    "        \n",
    "        # Plot the results\n",
    "        plt.plot(all_times, ratios, label=f\"{name} (janela={window_size}s)\")\n",
    "    \n",
    "    # Plot migration and disaster lines\n",
    "    plota_migracao_e_desastre()\n",
    "    \n",
    "    plt.xlabel('Segundos')\n",
    "    plt.ylabel('Taxa de Bloqueio')\n",
    "    plt.title(f'Taxa de Bloqueio (Janela Deslizante de {window_size} segundos)')\n",
    "    plt.grid(True)\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "def plot_avg_allocated_sum(link, dataframe, total_slots=200, time_step=0.1, window_size=10):\n",
    "    def __contem_link(caminho, link):\n",
    "        for i in range(len(caminho) - 1):\n",
    "            if (caminho[i], caminho[i + 1]) == link or (caminho[i + 1], caminho[i]) == link:\n",
    "                return True\n",
    "        return False\n",
    "    # Supondo que 'df' seja o seu DataFrame\n",
    "    filtered_dataframe = dataframe[dataframe[\"bloqueada\"] == False].copy()\n",
    "    filtered_dataframe['caminho'] = filtered_dataframe['caminho'].apply(eval)  # Convertendo as strings para listas\n",
    "    filtered_dataframe = filtered_dataframe[filtered_dataframe['caminho'].apply(lambda x: __contem_link(x, link))]\n",
    "\n",
    "    max_time = max(filtered_dataframe['tempo_desalocacao'].max(), filtered_dataframe['tempo_criacao'].max())\n",
    "    \n",
    "    # Initialize time points\n",
    "    time_points = np.arange(10, max_time + time_step, time_step)\n",
    "    \n",
    "    # Store allocated_sum for each time step\n",
    "    allocated_sums = []\n",
    "    \n",
    "    for t in time_points:\n",
    "        # Filter the DataFrame for the current time\n",
    "        current_requisitions = filtered_dataframe[\n",
    "            (filtered_dataframe['tempo_criacao'] <= t) & \n",
    "            (filtered_dataframe['tempo_desalocacao'] > t)\n",
    "        ]\n",
    "        \n",
    "        # Initialize slots for the current time\n",
    "        slots = np.zeros(total_slots, dtype=int)\n",
    "        \n",
    "        # Get allocated ranges and mark slots\n",
    "        allocated_requests = current_requisitions['index_de_inicio_e_final'].tolist()\n",
    "        allocated_requests = [eval(req) for req in allocated_requests]\n",
    "        \n",
    "        for start, end in allocated_requests:\n",
    "            slots[start:end+1] = 1  # +1 to include the last index\n",
    "        \n",
    "        # Calculate allocated_sum\n",
    "        allocated_sum = np.sum(slots)\n",
    "        allocated_sums.append(allocated_sum)\n",
    "    \n",
    "    # Calculate averages over the span of 10 seconds\n",
    "    window_points = np.arange(0, max_time + time_step, time_step)\n",
    "    avg_allocated_sums = []\n",
    "    for t in window_points:\n",
    "        # Define the window range\n",
    "        start_time = max(0, t - window_size)\n",
    "        end_time = t\n",
    "        \n",
    "        # Get indices within the window\n",
    "        indices_in_window = (time_points >= start_time) & (time_points <= end_time)\n",
    "        avg_allocated_sum = np.mean(np.array(allocated_sums)[indices_in_window])\n",
    "        avg_allocated_sums.append(avg_allocated_sum)\n",
    "    \n",
    "    # Plot the results\n",
    "    plota_migracao_e_desastre()\n",
    "    plt.plot(window_points, avg_allocated_sums, label='Quantidade média de slots alocados')\n",
    "    plt.title(f'Quantidade media de Slots de frequência alocados no link {link} (janela de {window_size} segundos, dados do link coletados a cada {time_step} segundos)')\n",
    "    plt.xlabel('Tempo (segundos)')\n",
    "    plt.ylabel('Media de Slots de Frequência Alocados')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def plot_largest_free_window(link, dataframe = dataframe1, total_slots=200, time_step=0.1, window_size=10):\n",
    "\n",
    "    # Filter for non-blocked paths and containing the desired link\n",
    "    filtered_dataframe = dataframe[dataframe[\"bloqueada\"] == False].copy()\n",
    "    filtered_dataframe['caminho'] = filtered_dataframe['caminho'].apply(eval)  # Convert strings to lists\n",
    "    filtered_dataframe = filtered_dataframe[filtered_dataframe['caminho'].apply(lambda x: contem_link(x, link))]\n",
    "\n",
    "    # Get the maximum time duration in the dataset\n",
    "    max_time = max(filtered_dataframe['tempo_desalocacao'].max(), filtered_dataframe['tempo_criacao'].max())\n",
    "\n",
    "    # Initialize time points\n",
    "    time_points = np.arange(10, max_time + time_step, time_step)\n",
    "\n",
    "    # Store the largest free window for each time step\n",
    "    largest_windows = []\n",
    "\n",
    "    for t in time_points:\n",
    "        # Filter the DataFrame for the current time\n",
    "        current_requisitions = filtered_dataframe[\n",
    "            (filtered_dataframe['tempo_criacao'] <= t) & \n",
    "            (filtered_dataframe['tempo_desalocacao'] > t)\n",
    "        ]\n",
    "\n",
    "        # Initialize slots for the current time\n",
    "        slots = np.zeros(total_slots, dtype=int)\n",
    "\n",
    "        # Get allocated ranges and mark slots\n",
    "        allocated_requests = current_requisitions['index_de_inicio_e_final'].tolist()\n",
    "        allocated_requests = [eval(req) for req in allocated_requests]\n",
    "\n",
    "        for start, end in allocated_requests:\n",
    "            slots[start:end+1] = 1  # +1 to include the last index\n",
    "\n",
    "        # Find the largest free window\n",
    "        unallocated_windows = ''.join(map(str, slots)).split('1')\n",
    "        largest_window = max(len(window) for window in unallocated_windows)\n",
    "        largest_windows.append(largest_window)\n",
    "\n",
    "    # Calculate averages over the span of 10 seconds\n",
    "    avg_largest_windows = []\n",
    "    for t in time_points:\n",
    "        # Define the window range\n",
    "        start_time = max(0, t - window_size)\n",
    "        end_time = t\n",
    "\n",
    "        # Get indices within the window\n",
    "        indices_in_window = (time_points >= start_time) & (time_points <= end_time)\n",
    "        avg_largest_window = np.mean(np.array(largest_windows)[indices_in_window])\n",
    "        avg_largest_windows.append(avg_largest_window)\n",
    "\n",
    "    # Plot the results\n",
    "    plota_migracao_e_desastre()\n",
    "    plt.plot(time_points, avg_largest_windows, label='Tamanho medio da maior janela livre')\n",
    "    plt.title(f'Tamanho medio da maior janela livre pelo tempo no link {link} (janela de {window_size}, coletada cada {time_step} segundos)')\n",
    "    plt.xlabel('Tempo segundos')\n",
    "    plt.ylabel('Tamanho medio da maior janela livre')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "simulador-eon-pre-desastre-cLZF9wpT-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
